requirements.txt

__pycache__
*.pyc
.git
.gitignore
README.md
plots/
data.xlsx


EDA.ipynb

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score,mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, cross_validate
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
import os
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.ensemble import AdaBoostRegressor, HistGradientBoostingRegressor


import mlflow
import mlflow.sklearn
os.environ['USER'] = 'Ruslan'
mlflow.set_tracking_uri("http://localhost:5005")

df = pd.read_excel(r'C:\Users\User\Desktop\oil\data.xlsx')

sns.set_style("whitegrid")
sns.set_palette("husl")  # –ö—Ä–∞—Å–∏–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ —Ü–≤–µ—Ç–æ–≤

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
numeric_cols = df.select_dtypes(include=[float, int]).columns

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–µ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ceil(sqrt(n)) –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–π —Ñ–æ—Ä–º—ã)
n_cols = len(numeric_cols)
n_rows = int(n_cols ** 0.5) + 1
n_cols_grid = int(n_cols / n_rows) + 1

fig, axes = plt.subplots(n_rows, n_cols_grid, figsize=(12, 8), sharex=False, sharey=False)
axes = axes.flatten()  # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ 1D-–º–∞—Å—Å–∏–≤ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã

# –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], bins=30, kde=True, ax=axes[i], alpha=0.7)  # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å KDE
    axes[i].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞' if i % n_cols_grid == 0 else '')  # –ü–æ–¥–ø–∏—Å—å —Ç–æ–ª—å–∫–æ —Å–ª–µ–≤–∞

# –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—É–±–ø–ª–æ—Ç—ã, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–æ–∫ –º–µ–Ω—å—à–µ —è—á–µ–µ–∫
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
plt.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫', fontsize=12, y=0.98)  # –û–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
plt.show()


corr=df.select_dtypes(include=['float','int']).corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞')
plt.show()

df.select_dtypes(include=['object']).value_counts()

x=df.drop(['cond rate','gas rate','sum cond','sum gas','NPV'],axis=1)
y=df['NPV']

# One-Hot Encoding –¥–ª—è GS
encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' –∏–∑–±–µ–≥–∞–µ—Ç –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
encoded_cols = encoder.fit_transform(x[['GS']])
encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['GS']))
X = pd.concat([x.drop('GS', axis=1), encoded_df], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

mlflow.set_experiment("Models_NPV")

from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

def test_model(model, param_grid, model_name, X_train, y_train, X_test, y_test):
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring='r2',
        n_jobs=-1,
        verbose=1
    )
    
    with mlflow.start_run(run_name=model_name):
        grid_search.fit(X_train, y_train)
        
        mlflow.log_params(grid_search.best_params_)
        print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {model_name}:", grid_search.best_params_)
        print(f"–õ—É—á—à–µ–µ R2 –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {grid_search.best_score_:.4f}")
        
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        r2_val = r2_score(y_test, y_pred)
        mape = mean_absolute_percentage_error(y_test, y_pred)
        
        mlflow.log_metric('MAE', mae)
        mlflow.log_metric('R2', r2_val)  # –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å r2 –Ω–∞ r2_val
        mlflow.log_metric('MAPE', mape)
        mlflow.sklearn.log_model(best_model, "best_model")
        
        print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {model_name}:")
        print(f"  MAE = {mae:.2f}")
        print(f"  R2  = {r2_val:.4f}")
        print(f"  MAPE = {mape:.2%}")
        print("-" * 50)
        
        return best_model, mape


#–º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
models_to_test = [
    # –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥)
    (XGBRegressor(random_state=42, objective='reg:squarederror'), 
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'subsample': [0.8, 0.9]}, "XGBoost"),
    
    (LGBMRegressor(random_state=42, verbose=-1), 
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'subsample': [0.8, 0.9]}, "LightGBM"),
    
    (CatBoostRegressor(random_state=42, verbose=False),
     {'iterations': [100, 500], 'learning_rate': [0.01, 0.1], 
      'depth': [4, 6], 'l2_leaf_reg': [1, 3]}, "CatBoost"),
    
    (GradientBoostingRegressor(random_state=42),
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [3, 4], 'subsample': [0.8, 0.9]}, "GradientBoosting"),
    
    (HistGradientBoostingRegressor(random_state=42),
     {'max_iter': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'l2_regularization': [0, 1]}, "HistGradientBoosting"),
    
    # –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã (–±—ç–≥–≥–∏–Ω–≥)
    (RandomForestRegressor(random_state=42),
     {'n_estimators': [100, 500], 'max_depth': [4, 6, None], 
      'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}, "RandomForest"),
    
    (ExtraTreesRegressor(random_state=42),
     {'n_estimators': [100, 500], 'max_depth': [4, 6, None], 
      'min_samples_split': [2, 5]}, "ExtraTrees"),
    
    # –î—Ä—É–≥–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
    (AdaBoostRegressor(random_state=42),
     {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1.0]}, "AdaBoost"),
    
    # –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
    (Ridge(random_state=42),
     {'alpha': [0.1, 1.0, 10.0, 100.0], 'solver': ['auto', 'svd']}, "Ridge"),
    
    (Lasso(random_state=42),
     {'alpha': [0.1, 1.0, 10.0], 'max_iter': [1000, 5000]}, "Lasso"),
    
    (ElasticNet(random_state=42),
     {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8]}, "ElasticNet"),
    
    (BayesianRidge(),
     {'alpha_1': [1e-6, 1e-5], 'alpha_2': [1e-6, 1e-5], 
      'lambda_1': [1e-6, 1e-5], 'lambda_2': [1e-6, 1e-5]}, "BayesianRidge"),
    
    # –ú–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å–µ–¥–µ–π
    (KNeighborsRegressor(),
     {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 
      'metric': ['euclidean', 'manhattan']}, "KNeighbors"),
    
    # SVM
    (SVR(),
     {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 
      'gamma': ['scale', 'auto'], 'epsilon': [0.1, 0.2]}, "SVR"),
    
    # –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
    (MLPRegressor(random_state=42, max_iter=1000),
     {'hidden_layer_sizes': [(100,), (50, 50), (100, 50)], 
      'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001],
      'learning_rate_init': [0.001, 0.01]}, "MLP"),
    
    # –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π
    (DecisionTreeRegressor(random_state=42),
     {'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10], 
      'min_samples_leaf': [1, 2, 4]}, "DecisionTree")
]

#–º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
models_to_test = [
    # SVM
    #(SVR(),
     #{'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 
      #'gamma': ['scale', 'auto'], 'epsilon': [0.1, 0.2]}, "SVR"),
    
    # –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
    (MLPRegressor(random_state=42, max_iter=1000),
     {'hidden_layer_sizes': [(100,), (50, 50), (100, 50)], 
      'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001],
      'learning_rate_init': [0.001, 0.01]}, "MLP"),
    
    # –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π
    (DecisionTreeRegressor(random_state=42),
     {'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10], 
      'min_samples_leaf': [1, 2, 4]}, "DecisionTree")
]

#—Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏–π
#best_models = {}
for model, params, name in models_to_test:
    best_model, mape = test_model(model, params, name, X_train, y_train, X_test, y_test)
    best_models[name] = (best_model, mape)
	
train_and_save.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score, cross_validate
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
import pickle  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ —ç–Ω–∫–æ–¥–µ—Ä–∞
import os  # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—É—Ç–µ–π
import joblib

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs('plots', exist_ok=True)

csv_path='data.xlsx'

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—É—Ç–∏
csv_path = os.getenv('DATA_PATH', 'data.xlsx')
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"–§–∞–π–ª {csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ volume –∏–ª–∏ –ø—É—Ç—å.")

df = pd.read_excel(csv_path)
if df.empty:
    raise ValueError("–î–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ!")
    
# –ö—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (—Ç–≤–æ–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥, –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
print("–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:", df.shape)
print("–û–ø–∏—Å–∞–Ω–∏–µ:", df.describe())
print("–ò–Ω—Ñ–æ:", df.info())
print("–ü—Ä–æ–ø—É—Å–∫–∏:", df.isnull().sum())
print("–ö–æ–ª–æ–Ω–∫–∏:", df.columns)

# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (—Ç–≤–æ–π –∫–æ–¥, –æ—Å—Ç–∞–≤–ª—è–µ–º)
sns.set_style("whitegrid")
sns.set_palette("husl")
numeric_cols = df.select_dtypes(include=[float, int]).columns
n_cols = len(numeric_cols)
n_rows = int(n_cols ** 0.5) + 1
n_cols_grid = int(n_cols / n_rows) + 1
fig, axes = plt.subplots(n_rows, n_cols_grid, figsize=(12, 8), sharex=False, sharey=False)
axes = axes.flatten()
for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], bins=30, kde=True, ax=axes[i], alpha=0.7)
    axes[i].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞' if i % n_cols_grid == 0 else '')
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫', fontsize=12, y=0.98)
plt.savefig('plots/histograms.png', dpi=300, bbox_inches='tight')
#plt.show()



# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (—Ç–≤–æ–π –∫–æ–¥)
corr = df.select_dtypes(include=['float', 'int']).corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞')
plt.savefig('plots/correlation_matrix.png', dpi=300, bbox_inches='tight')
#plt.show()

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
x = df.drop(['cond rate', 'gas rate', 'sum cond', 'sum gas', 'NPV'], axis=1)  # –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
y = df['NPV']  # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è

# One-Hot Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ GS
encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' —É–±–∏—Ä–∞–µ—Ç –æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
encoded_cols = encoder.fit_transform(x[['GS']])  # –û–±—É—á–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ GS
encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['GS']))
X = pd.concat([x.drop(['GS'], axis=1), encoded_df], axis=1)  # –§–∏–Ω–∞–ª—å–Ω—ã–π X —Å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

feature_columns = X.columns.tolist()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# –û–±—É—á–µ–Ω–∏–µ XGBoost —Å GridSearch
xgb = XGBRegressor(random_state=42, objective='reg:squarederror')
param_grid = {
    'n_estimators': [10, 50, 100, 200, 1000],
    'max_depth': [3, 5, 7, 15],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 1.0]
}
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print("–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:", grid_search.best_params_)
print(f"–õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ R2 –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {grid_search.best_score_:.4f}")

# –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏ (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ —Ç–≤–æ–µ–≥–æ –∫–æ–¥–∞)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è XGBoost: MAE={mae:.2f}, R2={r2:.2f}, MAPE={mape:.2%}")

# –ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ (—Ç–≤–æ–π –∫–æ–¥, –æ—Å—Ç–∞–≤–ª—è–µ–º)
plt.figure(figsize=(4, 4))
plt.scatter(y_test, y_pred, alpha=0.5, color='black')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è NPV')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è NPV')
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π')
plt.grid(True)
plt.savefig('plots/prediction_scatter.png', dpi=300, bbox_inches='tight')
#plt.show()

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_importances = best_model.feature_importances_
feature_names = X_train.columns
importance_df = pd.DataFrame({'–ü—Ä–∏–∑–Ω–∞–∫': feature_names, '–í–∞–∂–Ω–æ—Å—Ç—å': feature_importances}).sort_values(by='–í–∞–∂–Ω–æ—Å—Ç—å', ascending=False)
print("\n–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(importance_df.head(10))
plt.figure(figsize=(7, 3))
plt.barh(importance_df['–ü—Ä–∏–∑–Ω–∞–∫'][:20], importance_df['–í–∞–∂–Ω–æ—Å—Ç—å'][:20])
plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫')
plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ XGBoost')
plt.gca().invert_yaxis()
plt.savefig('plots/importance.png', dpi=300, bbox_inches='tight')
#plt.show()

os.makedirs('models', exist_ok=True)
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —ç–Ω–∫–æ–¥–µ—Ä–∞ –≤ .pkl —Ñ–∞–π–ª—ã (–∫–ª—é—á–µ–≤–æ–π —à–∞–≥ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞!)
joblib.dump(best_model, 'models/model.pkl')
joblib.dump(encoder, 'models/encoder.pkl')
joblib.dump(feature_columns, 'models/feature_columns.pkl') 
print("–ú–æ–¥–µ–ª—å,—ç–Ω–∫–æ–¥–µ—Ä –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ model.pkl –∏ encoder.pkl")


app.py
from fastapi import FastAPI, HTTPException
import joblib
import pandas as pd
from pydantic import BaseModel, Field
import numpy as np
import logging
from typing import List

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="NPV Prediction API", version="1.0")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —ç–Ω–∫–æ–¥–µ—Ä–∞
try:
    model = joblib.load('models/model.pkl')
    encoder = joblib.load('models/encoder.pkl')
    feature_columns = joblib.load('models/feature_columns.pkl')  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£
    logger.info("–ú–æ–¥–µ–ª—å –∏ —ç–Ω–∫–æ–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
    
    
# –ú–æ–¥–µ–ª—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
class InputData(BaseModel):
    Heff: float = Field(..., ge=0, description="–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤—ã—Å–æ—Ç–∞")
    Perm: float = Field(..., ge=0, description="–ü—Ä–æ–Ω–∏—Ü–∞–µ–º–æ—Å—Ç—å")
    Sg: float = Field(..., ge=0, le=1, description="–ì–∞–∑–æ–Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å")
    L_hor: float = Field(..., ge=0, description="–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞")
    GS: str = Field(..., description="–¢–∏–ø –≥–∞–∑–∞")
    temp: float = Field(..., ge=0, description="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞")
    C5: float = Field(..., ge=0, description="–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ C5")
    GRP: int = Field(..., ge=0, description="–ì—Ä—É–ø–ø–∞")
    nGS: int = Field(..., ge=0, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GS")
    
@app.get("/")
async def root():
    return {"message": "NPV Prediction API", "status": "active"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": True}

@app.post("/predict")
async def predict(data: InputData):
    try:
        input_dict = data.dict()
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ
        numeric_data = {k: v for k, v in input_dict.items() if k != 'GS'}
        
        # –ö–æ–¥–∏—Ä—É–µ–º GS
        gs_encoded = encoder.transform([[input_dict['GS']]])
        gs_columns = encoder.get_feature_names_out(['GS'])
        gs_data = dict(zip(gs_columns, gs_encoded[0]))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        all_data = {**numeric_data, **gs_data}
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º
        input_processed = pd.DataFrame([all_data])[feature_columns]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = model.predict(input_processed)
        result = float(prediction[0])
        
        return {"predicted_NPV": round(result, 2), "status": "success"}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")

@app.get("/model_info")
async def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        features = []
        if hasattr(model, 'feature_names_in_'):
            features = model.feature_names_in_.tolist()
        elif hasattr(model, 'get_booster'):
            features = model.get_booster().feature_names
        
        return {
            "model_type": type(model).__name__,
            "n_features": len(features),
            "features": features
        }
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
	
streamlit_app.py
import streamlit as st
import requests
import pandas as pd
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="NPV Prediction App",
    page_icon="üìä",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üí∞ NPV Prediction App")
st.markdown("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ NPV –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–∫–≤–∞–∂–∏–Ω—ã")

# URL –≤–∞—à–µ–≥–æ API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é localhost)
API_URL = st.sidebar.text_input(
    "URL API", 
    value="http://localhost:8000",
    help="–í–≤–µ–¥–∏—Ç–µ URL –≤–∞—à–µ–≥–æ FastAPI —Å–µ—Ä–≤–µ—Ä–∞"
)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å API
def check_api_health():
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞
def get_prediction(data):
    try:
        response = requests.post(
            f"{API_URL}/predict",
            json=data,
            timeout=10
        )
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}"}
    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
def get_model_info():
    try:
        response = requests.get(f"{API_URL}/model_info", timeout=5)
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except:
        return None

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å API
if st.sidebar.button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å API"):
    if check_api_health():
        st.sidebar.success("‚úÖ API –¥–æ—Å—Ç—É–ø–µ–Ω")
    else:
        st.sidebar.error("‚ùå API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–ª—è –≤–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö
st.header("üìù –í–≤–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

col1, col2 = st.columns(2)

with col1:
    st.subheader("–ì–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    heff = st.number_input("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤—ã—Å–æ—Ç–∞ (Heff)", min_value=0.0, value=10.0, step=0.1)
    perm = st.number_input("–ü—Ä–æ–Ω–∏—Ü–∞–µ–º–æ—Å—Ç—å (Perm)", min_value=0.0, value=100.0, step=1.0)
    sg = st.slider("–ì–∞–∑–æ–Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å (Sg)", min_value=0.0, max_value=1.0, value=0.8, step=0.01)
    c5 = st.number_input("–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ C5", min_value=0.0, value=0.5, step=0.1)

with col2:
    st.subheader("–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    l_hor = st.number_input("–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (L_hor)", min_value=0.0, value=500.0, step=10.0)
    gs = st.selectbox("–¢–∏–ø –ø—Ä–æ–≤–æ–¥–∫–∏ —Å—Ç–≤–æ–ª–∞", ["S-TYPE", "U-TYPE", "VGS", "GS", "NGS"])
    temp = st.number_input("–¢–µ–º–ø –ø–∞–¥–µ–Ω–∏—è", min_value=0.0, value=20.0, step=0.1)
    grp = st.number_input("—Å—Ç–∞–¥–∏–π –ì–†–ü)", min_value=0, value=1, step=1)
    ngs = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç–≤–æ–ª–æ–≤", min_value=0, value=2, step=1)

# –ö–Ω–æ–ø–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
if st.button("üéØ –†–∞—Å—Å—á–∏—Ç–∞—Ç—å NPV", type="primary"):
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è API
    input_data = {
        "Heff": heff,
        "Perm": perm,
        "Sg": sg,
        "L_hor": l_hor,
        "GS": gs,
        "temp": temp,
        "C5": c5,
        "GRP": grp,
        "nGS": ngs
    }
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞
    with st.spinner("–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞..."):
        result = get_prediction(input_data)
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if "predicted_NPV" in result:
        st.success(f"## –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π NPV: **{result['predicted_NPV']:,.2f}**")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        with st.expander("üìä –î–µ—Ç–∞–ª–∏ –∑–∞–ø—Ä–æ—Å–∞"):
            st.json(input_data)
            st.json(result)
    else:
        st.error(f"–û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
st.sidebar.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
model_info = get_model_info()
if model_info and "error" not in model_info:
    st.sidebar.write(f"**–¢–∏–ø –º–æ–¥–µ–ª–∏:** {model_info.get('model_type', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
    st.sidebar.write(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:** {model_info.get('n_features', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
    
    if "features" in model_info:
        with st.sidebar.expander("–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
            for feature in model_info["features"]:
                st.write(f"‚Ä¢ {feature}")
else:
    st.sidebar.info("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

# –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
st.sidebar.header("üìã –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤")
if st.sidebar.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä 1"):
    st.experimental_set_query_params(example=1)
    st.rerun()

if st.sidebar.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä 2"):
    st.experimental_set_query_params(example=2)
    st.rerun()

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
with st.expander("üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"):
    st.markdown("""
    1. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ FastAPI —Å–µ—Ä–≤–µ—Ä** –Ω–∞ –ø–æ—Ä—Ç—É 8000
    2. **–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ API –¥–æ—Å—Ç—É–ø–µ–Ω** (–∫–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è)
    3. **–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** —Å–∫–≤–∞–∂–∏–Ω—ã
    4. **–ù–∞–∂–º–∏—Ç–µ '–†–∞—Å—Å—á–∏—Ç–∞—Ç—å NPV'** –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞
    5. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã** –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
    
    **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤–∞—à Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å API –∑–∞–ø—É—â–µ–Ω!
    """)

# –§—É—Ç–µ—Ä
st.markdown("---")
st.caption("NPV Prediction App ‚Ä¢ Powered by FastAPI + Streamlit + XGBoost")

Dickerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
COPY models/ ./models/
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]


.gitignore
model.pkl
encoder.pkl
data.xlsx
__pycache__/
.env


.dockerignore
__pycache__
*.pyc
.git
.gitignore
README.md
plots/
data.xlsx

