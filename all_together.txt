requirements.txt

__pycache__
*.pyc
.git
.gitignore
README.md
plots/
data.xlsx


EDA.ipynb

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score,mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, cross_validate
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
import os
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.ensemble import AdaBoostRegressor, HistGradientBoostingRegressor


import mlflow
import mlflow.sklearn
os.environ['USER'] = 'Ruslan'
mlflow.set_tracking_uri("http://localhost:5005")

df = pd.read_excel(r'C:\Users\User\Desktop\oil\data.xlsx')

sns.set_style("whitegrid")
sns.set_palette("husl")  # Красивая палитра цветов

# Фильтруем только числовые колонки
numeric_cols = df.select_dtypes(include=[float, int]).columns

# Определяем размер сетки (например, ceil(sqrt(n)) для квадратной формы)
n_cols = len(numeric_cols)
n_rows = int(n_cols ** 0.5) + 1
n_cols_grid = int(n_cols / n_rows) + 1

fig, axes = plt.subplots(n_rows, n_cols_grid, figsize=(12, 8), sharex=False, sharey=False)
axes = axes.flatten()  # Превращаем в 1D-массив для простоты

# Строим графики
for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], bins=30, kde=True, ax=axes[i], alpha=0.7)  # Гистограмма с KDE
    axes[i].set_title(f'Распределение {col}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Частота' if i % n_cols_grid == 0 else '')  # Подпись только слева

# Убираем пустые субплоты, если колонок меньше ячеек
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()  # Автоматическая корректировка расстояний
plt.suptitle('Распределения числовых колонок', fontsize=12, y=0.98)  # Общий заголовок
plt.show()


corr=df.select_dtypes(include=['float','int']).corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Корреляционная матрица')
plt.show()

df.select_dtypes(include=['object']).value_counts()

x=df.drop(['cond rate','gas rate','sum cond','sum gas','NPV'],axis=1)
y=df['NPV']

# One-Hot Encoding для GS
encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' избегает мультиколлинеарности
encoded_cols = encoder.fit_transform(x[['GS']])
encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['GS']))
X = pd.concat([x.drop('GS', axis=1), encoded_df], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

mlflow.set_experiment("Models_NPV")

from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error

def test_model(model, param_grid, model_name, X_train, y_train, X_test, y_test):
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring='r2',
        n_jobs=-1,
        verbose=1
    )
    
    with mlflow.start_run(run_name=model_name):
        grid_search.fit(X_train, y_train)
        
        mlflow.log_params(grid_search.best_params_)
        print(f"Лучшие параметры для {model_name}:", grid_search.best_params_)
        print(f"Лучшее R2 на кросс-валидации: {grid_search.best_score_:.4f}")
        
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        r2_val = r2_score(y_test, y_pred)
        mape = mean_absolute_percentage_error(y_test, y_pred)
        
        mlflow.log_metric('MAE', mae)
        mlflow.log_metric('R2', r2_val)  # исправлено с r2 на r2_val
        mlflow.log_metric('MAPE', mape)
        mlflow.sklearn.log_model(best_model, "best_model")
        
        print(f"Тестовые метрики для {model_name}:")
        print(f"  MAE = {mae:.2f}")
        print(f"  R2  = {r2_val:.4f}")
        print(f"  MAPE = {mape:.2%}")
        print("-" * 50)
        
        return best_model, mape


#модели для теста
models_to_test = [
    # Ансамблевые методы (градиентный бустинг)
    (XGBRegressor(random_state=42, objective='reg:squarederror'), 
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'subsample': [0.8, 0.9]}, "XGBoost"),
    
    (LGBMRegressor(random_state=42, verbose=-1), 
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'subsample': [0.8, 0.9]}, "LightGBM"),
    
    (CatBoostRegressor(random_state=42, verbose=False),
     {'iterations': [100, 500], 'learning_rate': [0.01, 0.1], 
      'depth': [4, 6], 'l2_leaf_reg': [1, 3]}, "CatBoost"),
    
    (GradientBoostingRegressor(random_state=42),
     {'n_estimators': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [3, 4], 'subsample': [0.8, 0.9]}, "GradientBoosting"),
    
    (HistGradientBoostingRegressor(random_state=42),
     {'max_iter': [100, 500], 'learning_rate': [0.01, 0.1], 
      'max_depth': [4, 6], 'l2_regularization': [0, 1]}, "HistGradientBoosting"),
    
    # Ансамблевые методы (бэггинг)
    (RandomForestRegressor(random_state=42),
     {'n_estimators': [100, 500], 'max_depth': [4, 6, None], 
      'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}, "RandomForest"),
    
    (ExtraTreesRegressor(random_state=42),
     {'n_estimators': [100, 500], 'max_depth': [4, 6, None], 
      'min_samples_split': [2, 5]}, "ExtraTrees"),
    
    # Другие ансамблевые методы
    (AdaBoostRegressor(random_state=42),
     {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1.0]}, "AdaBoost"),
    
    # Линейные модели
    (Ridge(random_state=42),
     {'alpha': [0.1, 1.0, 10.0, 100.0], 'solver': ['auto', 'svd']}, "Ridge"),
    
    (Lasso(random_state=42),
     {'alpha': [0.1, 1.0, 10.0], 'max_iter': [1000, 5000]}, "Lasso"),
    
    (ElasticNet(random_state=42),
     {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8]}, "ElasticNet"),
    
    (BayesianRidge(),
     {'alpha_1': [1e-6, 1e-5], 'alpha_2': [1e-6, 1e-5], 
      'lambda_1': [1e-6, 1e-5], 'lambda_2': [1e-6, 1e-5]}, "BayesianRidge"),
    
    # Методы на основе соседей
    (KNeighborsRegressor(),
     {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 
      'metric': ['euclidean', 'manhattan']}, "KNeighbors"),
    
    # SVM
    (SVR(),
     {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 
      'gamma': ['scale', 'auto'], 'epsilon': [0.1, 0.2]}, "SVR"),
    
    # Нейронные сети
    (MLPRegressor(random_state=42, max_iter=1000),
     {'hidden_layer_sizes': [(100,), (50, 50), (100, 50)], 
      'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001],
      'learning_rate_init': [0.001, 0.01]}, "MLP"),
    
    # Деревья решений
    (DecisionTreeRegressor(random_state=42),
     {'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10], 
      'min_samples_leaf': [1, 2, 4]}, "DecisionTree")
]

#модели для теста
models_to_test = [
    # SVM
    #(SVR(),
     #{'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 
      #'gamma': ['scale', 'auto'], 'epsilon': [0.1, 0.2]}, "SVR"),
    
    # Нейронные сети
    (MLPRegressor(random_state=42, max_iter=1000),
     {'hidden_layer_sizes': [(100,), (50, 50), (100, 50)], 
      'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001],
      'learning_rate_init': [0.001, 0.01]}, "MLP"),
    
    # Деревья решений
    (DecisionTreeRegressor(random_state=42),
     {'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10], 
      'min_samples_leaf': [1, 2, 4]}, "DecisionTree")
]

#цикл обучений
#best_models = {}
for model, params, name in models_to_test:
    best_model, mape = test_model(model, params, name, X_train, y_train, X_test, y_test)
    best_models[name] = (best_model, mape)
	
train_and_save.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score, cross_validate
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
import pickle  # Для сохранения модели и энкодера
import os  # Для проверки путей
import joblib

# Создаем папку для графиков, если она не существует
os.makedirs('plots', exist_ok=True)

csv_path='data.xlsx'

# Используйте переменную окружения для пути
csv_path = os.getenv('DATA_PATH', 'data.xlsx')
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"Файл {csv_path} не найден! Проверьте volume или путь.")

df = pd.read_excel(csv_path)
if df.empty:
    raise ValueError("Данные пустые!")
    
# Краткий анализ данных (твой исходный код, оставляем для проверки)
print("Размер данных:", df.shape)
print("Описание:", df.describe())
print("Инфо:", df.info())
print("Пропуски:", df.isnull().sum())
print("Колонки:", df.columns)

# Гистограммы для числовых колонок (твой код, оставляем)
sns.set_style("whitegrid")
sns.set_palette("husl")
numeric_cols = df.select_dtypes(include=[float, int]).columns
n_cols = len(numeric_cols)
n_rows = int(n_cols ** 0.5) + 1
n_cols_grid = int(n_cols / n_rows) + 1
fig, axes = plt.subplots(n_rows, n_cols_grid, figsize=(12, 8), sharex=False, sharey=False)
axes = axes.flatten()
for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], bins=30, kde=True, ax=axes[i], alpha=0.7)
    axes[i].set_title(f'Распределение {col}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Частота' if i % n_cols_grid == 0 else '')
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.suptitle('Распределения числовых колонок', fontsize=12, y=0.98)
plt.savefig('plots/histograms.png', dpi=300, bbox_inches='tight')
#plt.show()



# Корреляционная матрица (твой код)
corr = df.select_dtypes(include=['float', 'int']).corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Корреляционная матрица')
plt.savefig('plots/correlation_matrix.png', dpi=300, bbox_inches='tight')
#plt.show()

# Подготовка данных
x = df.drop(['cond rate', 'gas rate', 'sum cond', 'sum gas', 'NPV'], axis=1)  # Входные признаки
y = df['NPV']  # Целевая переменная

# One-Hot Encoding для категориальной колонки GS
encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' убирает одну колонку, чтобы избежать мультиколлинеарности
encoded_cols = encoder.fit_transform(x[['GS']])  # Обучаем энкодер на GS
encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['GS']))
X = pd.concat([x.drop(['GS'], axis=1), encoded_df], axis=1)  # Финальный X с закодированными данными

feature_columns = X.columns.tolist()  # Сохраняем порядок колонок

# Разделение на train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучение XGBoost с GridSearch
xgb = XGBRegressor(random_state=42, objective='reg:squarederror')
param_grid = {
    'n_estimators': [10, 50, 100, 200, 1000],
    'max_depth': [3, 5, 7, 15],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 1.0]
}
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print("Лучшие параметры:", grid_search.best_params_)
print(f"Лучшее значение R2 на кросс-валидации: {grid_search.best_score_:.4f}")

# Лучшая модель
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Метрики (убираем дублирование из твоего кода)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"Тестовые метрики для XGBoost: MAE={mae:.2f}, R2={r2:.2f}, MAPE={mape:.2%}")

# Графики анализа (твой код, оставляем)
plt.figure(figsize=(4, 4))
plt.scatter(y_test, y_pred, alpha=0.5, color='black')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Фактические значения NPV')
plt.ylabel('Предсказанные значения NPV')
plt.title('Сравнение фактических и предсказанных значений')
plt.grid(True)
plt.savefig('plots/prediction_scatter.png', dpi=300, bbox_inches='tight')
#plt.show()

# Важность признаков
feature_importances = best_model.feature_importances_
feature_names = X_train.columns
importance_df = pd.DataFrame({'Признак': feature_names, 'Важность': feature_importances}).sort_values(by='Важность', ascending=False)
print("\nТоп-10 важных признаков:")
print(importance_df.head(10))
plt.figure(figsize=(7, 3))
plt.barh(importance_df['Признак'][:20], importance_df['Важность'][:20])
plt.xlabel('Важность')
plt.ylabel('Признак')
plt.title('Важность признаков в XGBoost')
plt.gca().invert_yaxis()
plt.savefig('plots/importance.png', dpi=300, bbox_inches='tight')
#plt.show()

os.makedirs('models', exist_ok=True)
# Сохранение модели и энкодера в .pkl файлы (ключевой шаг для продакшна!)
joblib.dump(best_model, 'models/model.pkl')
joblib.dump(encoder, 'models/encoder.pkl')
joblib.dump(feature_columns, 'models/feature_columns.pkl') 
print("Модель,энкодер и последовательность признаков сохранены в model.pkl и encoder.pkl")


app.py
from fastapi import FastAPI, HTTPException
import joblib
import pandas as pd
from pydantic import BaseModel, Field
import numpy as np
import logging
from typing import List

# Настройка логирования
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="NPV Prediction API", version="1.0")

# Загрузка модели и энкодера
try:
    model = joblib.load('models/model.pkl')
    encoder = joblib.load('models/encoder.pkl')
    feature_columns = joblib.load('models/feature_columns.pkl')  # ← ДОБАВИТЬ ЭТУ СТРОКУ
    logger.info("Модель и энкодер успешно загружены")
except Exception as e:
    logger.error(f"Ошибка загрузки модели: {e}")
    raise RuntimeError("Не удалось загрузить модель")
    
    
# Модель входных данных
class InputData(BaseModel):
    Heff: float = Field(..., ge=0, description="Эффективная высота")
    Perm: float = Field(..., ge=0, description="Проницаемость")
    Sg: float = Field(..., ge=0, le=1, description="Газонасыщенность")
    L_hor: float = Field(..., ge=0, description="Горизонтальная длина")
    GS: str = Field(..., description="Тип газа")
    temp: float = Field(..., ge=0, description="Температура")
    C5: float = Field(..., ge=0, description="Содержание C5")
    GRP: int = Field(..., ge=0, description="Группа")
    nGS: int = Field(..., ge=0, description="Количество GS")
    
@app.get("/")
async def root():
    return {"message": "NPV Prediction API", "status": "active"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": True}

@app.post("/predict")
async def predict(data: InputData):
    try:
        input_dict = data.dict()
        
        # Создаем DataFrame с правильным порядком изначально
        numeric_data = {k: v for k, v in input_dict.items() if k != 'GS'}
        
        # Кодируем GS
        gs_encoded = encoder.transform([[input_dict['GS']]])
        gs_columns = encoder.get_feature_names_out(['GS'])
        gs_data = dict(zip(gs_columns, gs_encoded[0]))
        
        # Объединяем данные
        all_data = {**numeric_data, **gs_data}
        
        # Создаем DataFrame с правильным порядком
        input_processed = pd.DataFrame([all_data])[feature_columns]
        
        # Предсказание
        prediction = model.predict(input_processed)
        result = float(prediction[0])
        
        return {"predicted_NPV": round(result, 2), "status": "success"}
        
    except Exception as e:
        logger.error(f"Ошибка предсказания: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ошибка предсказания: {str(e)}")

@app.get("/model_info")
async def model_info():
    """Информация о загруженной модели"""
    try:
        features = []
        if hasattr(model, 'feature_names_in_'):
            features = model.feature_names_in_.tolist()
        elif hasattr(model, 'get_booster'):
            features = model.get_booster().feature_names
        
        return {
            "model_type": type(model).__name__,
            "n_features": len(features),
            "features": features
        }
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
	
streamlit_app.py
import streamlit as st
import requests
import pandas as pd
import json

# Настройки страницы
st.set_page_config(
    page_title="NPV Prediction App",
    page_icon="📊",
    layout="wide"
)

# Заголовок приложения
st.title("💰 NPV Prediction App")
st.markdown("Прогнозирование NPV на основе параметров скважины")

# URL вашего API (по умолчанию localhost)
API_URL = st.sidebar.text_input(
    "URL API", 
    value="http://localhost:8000",
    help="Введите URL вашего FastAPI сервера"
)

# Функция для проверки соединения с API
def check_api_health():
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

# Функция для получения прогноза
def get_prediction(data):
    try:
        response = requests.post(
            f"{API_URL}/predict",
            json=data,
            timeout=10
        )
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Ошибка API: {response.status_code} - {response.text}"}
    except Exception as e:
        return {"error": f"Ошибка соединения: {str(e)}"}

# Функция для получения информации о модели
def get_model_info():
    try:
        response = requests.get(f"{API_URL}/model_info", timeout=5)
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except:
        return None

# Проверка соединения с API
if st.sidebar.button("Проверить соединение с API"):
    if check_api_health():
        st.sidebar.success("✅ API доступен")
    else:
        st.sidebar.error("❌ API недоступен")

# Основная форма для ввода данных
st.header("📝 Ввод параметров")

col1, col2 = st.columns(2)

with col1:
    st.subheader("Геологические параметры")
    heff = st.number_input("Эффективная высота (Heff)", min_value=0.0, value=10.0, step=0.1)
    perm = st.number_input("Проницаемость (Perm)", min_value=0.0, value=100.0, step=1.0)
    sg = st.slider("Газонасыщенность (Sg)", min_value=0.0, max_value=1.0, value=0.8, step=0.01)
    c5 = st.number_input("Содержание C5", min_value=0.0, value=0.5, step=0.1)

with col2:
    st.subheader("Технологические параметры")
    l_hor = st.number_input("Горизонтальная длина (L_hor)", min_value=0.0, value=500.0, step=10.0)
    gs = st.selectbox("Тип проводки ствола", ["S-TYPE", "U-TYPE", "VGS", "GS", "NGS"])
    temp = st.number_input("Темп падения", min_value=0.0, value=20.0, step=0.1)
    grp = st.number_input("стадий ГРП)", min_value=0, value=1, step=1)
    ngs = st.number_input("Количество горизонтальных стволов", min_value=0, value=2, step=1)

# Кнопка предсказания
if st.button("🎯 Рассчитать NPV", type="primary"):
    # Подготовка данных для API
    input_data = {
        "Heff": heff,
        "Perm": perm,
        "Sg": sg,
        "L_hor": l_hor,
        "GS": gs,
        "temp": temp,
        "C5": c5,
        "GRP": grp,
        "nGS": ngs
    }
    
    # Получение прогноза
    with st.spinner("Получение прогноза..."):
        result = get_prediction(input_data)
    
    # Отображение результатов
    if "predicted_NPV" in result:
        st.success(f"## Прогнозируемый NPV: **{result['predicted_NPV']:,.2f}**")
        
        # Дополнительная информация
        with st.expander("📊 Детали запроса"):
            st.json(input_data)
            st.json(result)
    else:
        st.error(f"Ошибка: {result.get('error', 'Неизвестная ошибка')}")

# Информация о модели
st.sidebar.header("ℹ️ Информация о модели")
model_info = get_model_info()
if model_info and "error" not in model_info:
    st.sidebar.write(f"**Тип модели:** {model_info.get('model_type', 'Неизвестно')}")
    st.sidebar.write(f"**Количество признаков:** {model_info.get('n_features', 'Неизвестно')}")
    
    if "features" in model_info:
        with st.sidebar.expander("Список признаков"):
            for feature in model_info["features"]:
                st.write(f"• {feature}")
else:
    st.sidebar.info("Информация о модели недоступна")

# Примеры запросов
st.sidebar.header("📋 Примеры запросов")
if st.sidebar.button("Загрузить пример 1"):
    st.experimental_set_query_params(example=1)
    st.rerun()

if st.sidebar.button("Загрузить пример 2"):
    st.experimental_set_query_params(example=2)
    st.rerun()

# Инструкция
with st.expander("📖 Инструкция по использованию"):
    st.markdown("""
    1. **Запустите FastAPI сервер** на порту 8000
    2. **Убедитесь что API доступен** (кнопка проверки соединения)
    3. **Заполните все параметры** скважины
    4. **Нажмите 'Рассчитать NPV'** для получения прогноза
    5. **Используйте примеры** для быстрого заполнения
    
    **Примечание:** Убедитесь что ваш Docker контейнер с API запущен!
    """)

# Футер
st.markdown("---")
st.caption("NPV Prediction App • Powered by FastAPI + Streamlit + XGBoost")

Dickerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
COPY models/ ./models/
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]


.gitignore
model.pkl
encoder.pkl
data.xlsx
__pycache__/
.env


.dockerignore
__pycache__
*.pyc
.git
.gitignore
README.md
plots/
data.xlsx

