Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ² Ğ±Ğ°Ñˆ
find . -type d \( -name 'oil' -o -name '.dvc' -o -name '.git' -o -name '__pycache__' \) -prune -o -type f -printf '%h/%f\n' | head -30


Ğ”Ğ°Ğ»ĞµĞµ Ğ²ĞµÑÑŒ ĞºĞ¾Ğ´ 

# Ğ¡ĞµÑ€Ğ²Ğ¸Ñ FastAPI Ğ´Ğ»Ñ API Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ NPV
  api:
    build: .  # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ· Ğ¸Ğ· Dockerfile Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    ports:
      - "8001:8001"  # ĞŸĞ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº API (http://localhost:8001/docs Ğ´Ğ»Ñ Swagger)
    command: >  # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¾: Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ DVC Ğ¿ĞµÑ€ĞµĞ´ API
      bash -c "
        echo 'Ğ—Ğ°Ğ¿ÑƒÑĞº DVC Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°...' &&
        dvc repro &&  # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²ĞµÑÑŒ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ DVC
        echo 'DVC Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½. Ğ—Ğ°Ğ¿ÑƒÑĞº API...' &&
        uvicorn app:app --host 0.0.0.0 --port 8001  # Ğ—Ğ°Ğ¿ÑƒÑĞº FastAPI
      "
    depends_on:
      - mlflow  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ MLflow (Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ¿Ğ° register)
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000  # Ğ”Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² MLflow Ğ¸Ğ· API
    volumes:
      - .:/app  # ĞœĞ¾Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ (Ğ´Ğ»Ñ DVC Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
      - /app/__pycache__  # Ğ˜ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºÑÑˆ Python


  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Streamlit Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°
  streamlit:
    build: .  # Ğ¢Ğ¾Ñ‚ Ğ¶Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·
    ports:
      - "8501:8501"  # ĞŸĞ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº UI (http://localhost:8501)
    command: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0  # Ğ—Ğ°Ğ¿ÑƒÑĞº Streamlit
    depends_on:
      - api  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ API
    environment:
      - API_URL=http://api:8001  # Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğº API Ğ¸Ğ· Streamlit
      - MLFLOW_TRACKING_URI=http://mlflow:5000  # Ğ”Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ MLflow
    volumes:
      - .:/app  # ĞœĞ¾Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚
      - /app/__pycache__

volumes:
  mlflow_data:  # Volume Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MLflow (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ñ‚ĞµÑ€ÑĞ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./Dockerfile
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FROM python:3.9-slim

WORKDIR /app

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° DVC Ğ¸ git (Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ DVC)
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

# ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ requirements Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN pip install dvc


# ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°
COPY . .

# Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ¿Ğ¾Ñ€Ñ‚Ğ°
EXPOSE 8001

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8001"]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./Dockerfile_orig
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FROM python:3.9-slim

WORKDIR /app

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° DVC Ğ¸ git (Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ DVC)
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

# ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ requirements Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN pip install dvc

# ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°
COPY . .

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ DVC (Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞµĞ½Ğ° Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ remote storage)
RUN dvc init --no-scm

# Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ¿Ğ¾Ñ€Ñ‚Ğ°
EXPOSE 8000

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./dvc.lock
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
schema: '2.0'
stages:
  preprocess:
    cmd: python src/preprocess.py
    deps:
    - path: data/raw/data.xlsx
      hash: md5
      md5: 5878791f8961af5143d726b8a2964c0f
      size: 1055731
    - path: params.yaml
      hash: md5
      md5: 57963591351d7e820582b02e0296c81e
      size: 459
    - path: src/preprocess.py
      hash: md5
      md5: edb5325804150f45627fe4df806d0d74
      size: 2041
    params:
      params.yaml:
        data.processed_path: data/processed/train_test.joblib
        data.raw_path: data/raw/data.xlsx
        features.categorical_columns:
        - GS
        features.drop_columns:
        - cond rate
        - gas rate
        - sum cond
        - sum gas
        features.target: NPV
        preprocessing.random_state: 42
        preprocessing.test_size: 0.2
    outs:
    - path: data/processed/train_test.joblib
      hash: md5
      md5: 7529128e1d84fb367ab8192fbea58247
      size: 928180
    - path: models/encoder.joblib
      hash: md5
      md5: 97a5994e25d699c849c80f9c73a8db1b
      size: 1389
    - path: models/feature_columns.joblib
      hash: md5
      md5: bbb3a22cce65e92d97561c19fee2f343
      size: 109
  train:
    cmd: python src/train.py
    deps:
    - path: data/processed/train_test.joblib
      hash: md5
      md5: 7529128e1d84fb367ab8192fbea58247
      size: 928180
    - path: params.yaml
      hash: md5
      md5: 57963591351d7e820582b02e0296c81e
      size: 459
    - path: src/train.py
      hash: md5
      md5: 2aff6ed8bea61de1ec7c8760e1164f84
      size: 2413
    params:
      params.yaml:
        model.hyperparameters:
          n_estimators: 250
          max_depth: 15
          learning_rate: 0.01
          subsample: 0.8
        model.name: xgboost
        training.cv_folds: 5
        training.scoring: r2
    outs:
    - path: models/metrics.json
      hash: md5
      md5: f566893d9b8357c4236025968d0f550c
      size: 164
    - path: models/model.joblib
      hash: md5
      md5: 9f1f54ba1d588ba30c039498a16294fb
      size: 5068390
  evaluate:
    cmd: python src/evaluate.py
    deps:
    - path: data/processed/train_test.joblib
      hash: md5
      md5: 7529128e1d84fb367ab8192fbea58247
      size: 928180
    - path: models/model.joblib
      hash: md5
      md5: 9f1f54ba1d588ba30c039498a16294fb
      size: 5068390
    - path: src/evaluate.py
      hash: md5
      md5: d188b0517ac9098e19eaccf70bc0a79e
      size: 1638
    outs:
    - path: models/evaluation.json
      hash: md5
      md5: 50cd2ce43364f711a22b6c6337af649a
      size: 437
  generate_report:
    cmd: python src/generate_report.py
    deps:
    - path: models/evaluation.json
      hash: md5
      md5: 50cd2ce43364f711a22b6c6337af649a
      size: 437
    - path: models/metrics.json
      hash: md5
      md5: f566893d9b8357c4236025968d0f550c
      size: 164
    - path: params.yaml
      hash: md5
      md5: 57963591351d7e820582b02e0296c81e
      size: 459
    - path: src/generate_report.py
      hash: md5
      md5: c560a7a9c25118b5036412f52e0e49d0
      size: 1712
    outs:
    - path: reports/model_report.json
      hash: md5
      md5: aae02539421c6887722fdd05657eab56
      size: 782
  register:
    cmd: python src/register_model.py
    deps:
    - path: models/model.joblib
      hash: md5
      md5: 9f1f54ba1d588ba30c039498a16294fb
      size: 5068390
    - path: params.yaml
      hash: md5
      md5: 57963591351d7e820582b02e0296c81e
      size: 459
    - path: src/register_model.py
      hash: md5
      md5: e1cc9c50645a29659d97cc4835cfd39d
      size: 3861
    params:
      params.yaml:
        model.name: xgboost
    outs:
    - path: registry/model_info.json
      hash: md5
      md5: a430cbd26953353ac7a267047e8bdf2f
      size: 334


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./dvc.yaml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
stages:
  preprocess:
    cmd: python src/preprocess.py
    deps:
      - data/raw/data.xlsx
      - src/preprocess.py
      - params.yaml
    params:
      - data.raw_path
      - data.processed_path
      - features.target
      - features.drop_columns
      - features.categorical_columns
      - preprocessing.test_size
      - preprocessing.random_state
    outs:
      - data/processed/train_test.joblib
      - models/encoder.joblib
      - models/feature_columns.joblib

  train:
    cmd: python src/train.py
    deps:
      - data/processed/train_test.joblib
      - src/train.py
      - params.yaml
    params:
      - model.name
      - model.hyperparameters
      - training.cv_folds
      - training.scoring
    outs:
      - models/model.joblib
    metrics:
      - models/metrics.json:
          cache: false

  evaluate:
    cmd: python src/evaluate.py
    deps:
      - data/processed/train_test.joblib
      - models/model.joblib
      - src/evaluate.py
    metrics:
      - models/evaluation.json:
          cache: false

  generate_report:
    cmd: python src/generate_report.py
    deps:
      - models/metrics.json
      - models/evaluation.json
      - params.yaml
      - src/generate_report.py
    outs:
      - reports/model_report.json

  register:
    cmd: python src/register_model.py
    deps:
      - src/register_model.py
      - params.yaml
      - models/model.joblib
    params:
      - model.name
    metrics:
      - registry/model_info.json:
          cache: false

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./models/.gitignore
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/model.joblib


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./params.yaml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
data:
  raw_path: "data/raw/data.xlsx"
  processed_path: "data/processed/train_test.joblib"

features:
  target: "NPV"
  drop_columns: ["cond rate", "gas rate", "sum cond", "sum gas"]
  categorical_columns: ["GS"]

preprocessing:
  test_size: 0.2
  random_state: 42

model:
  name: "xgboost"
  hyperparameters:
    n_estimators: 250
    max_depth: 15
    learning_rate: 0.01
    subsample: 0.8

training:
  cv_folds: 5
  scoring: "r2"


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./reports/.gitignore
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/model_report.json


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./requirements.txt
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Core Data Science
numpy==1.26.4  # Ğ¤Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ° 1.26.4 (Ğ±Ñ‹Ğ»Ğ¾ 1.26.0, Ğ½Ğ¾ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑĞ²Ğ½Ğ¾)
xgboost==2.0.3  # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ´Ğ¾ 2.0.3 Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
pandas==2.0.3   # Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ pandas ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ (pandas 2.x Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ NumPy 1.26)
scikit-learn==1.3.0  # Ğ£Ğ¶Ğµ Ğ¾Ğº

# ML Operations & Tracking
mlflow==2.9.2
dvc==3.32.0

# Web Framework & API
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0

# Web Interface
streamlit==1.28.0
requests==2.31.0

# Data Processing & Serialization
joblib==1.3.2
openpyxl==3.1.2
pyyaml==6.0.1

# Visualization (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ² Ğ½Ğ¾ÑƒÑ‚Ğ±ÑƒĞºĞ°Ñ…)
matplotlib==3.8.0
seaborn==0.13.0

# Production Server (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
gunicorn==21.2.0

# Development (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ)
jupyter==1.0.0
ipython==8.17.2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./src/evaluate.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import joblib
import json
import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
import yaml

def load_params():
    with open("params.yaml", "r") as f:
        return yaml.safe_load(f)

def main():
    params = load_params()

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    data = joblib.load(params['data']['processed_path'])
    model = joblib.load('models/model.joblib')

    X_test, y_test = data['X_test'], data['y_test']

    # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
    y_pred = model.predict(X_test)

    # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°
    evaluation = {
        'test_metrics': {
            'mae': mean_absolute_error(y_test, y_pred),
            'r2': r2_score(y_test, y_pred),
            'mape': mean_absolute_percentage_error(y_test, y_pred)
        },
        'predictions_stats': {
            'actual_mean': float(y_test.mean()),
            'predicted_mean': float(y_pred.mean()),
            'actual_std': float(y_test.std()),
            'predicted_std': float(y_pred.std())
        },
        'residuals_analysis': {
            'residuals_mean': float((y_test - y_pred).mean()),
            'residuals_std': float((y_test - y_pred).std())
        }
    }

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
    with open('models/evaluation.json', 'w') as f:
        json.dump(evaluation, f, indent=2)

    print("âœ… Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
    print(f"RÂ² Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ: {evaluation['test_metrics']['r2']:.4f}")

if __name__ == "__main__":
    main()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./src/generate_report.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import json
import yaml
import pandas as pd
from datetime import datetime

def generate_report():
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    with open('models/metrics.json', 'r') as f:
        metrics = json.load(f)

    with open('models/evaluation.json', 'r') as f:
        evaluation = json.load(f)

    with open('params.yaml', 'r') as f:
        params = yaml.safe_load(f)

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
    report = {
        'timestamp': datetime.now().isoformat(),
        'model_info': {
            'name': params['model']['name'],
            'hyperparameters': params['model']['hyperparameters']
        },
        'performance': {
            'cross_validation': {
                'mean_r2': metrics['cv_mean'],
                'std_r2': metrics['cv_std']
            },
            'test_set': evaluation['test_metrics'],
            'predictions_quality': evaluation['predictions_stats']
        },
        'data_info': {
            'target': params['features']['target'],
            'features_count': len(params['features']['drop_columns']) + 1  # Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
        }
    }

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
    with open('reports/model_report.json', 'w') as f:
        json.dump(report, f, indent=2)

    print("ğŸ“Š ĞÑ‚Ñ‡ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½:")
    print(f"   ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {report['model_info']['name']}")
    print(f"   CV RÂ²: {report['performance']['cross_validation']['mean_r2']:.4f} Â± {report['performance']['cross_validation']['std_r2']:.4f}")
    print(f"   Test RÂ²: {report['performance']['test_set']['r2']:.4f}")

if __name__ == "__main__":
    generate_report()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./src/preprocess.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import pandas as pd
import joblib
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import yaml
import os

def load_params():
    with open("params.yaml", "r") as f:
        return yaml.safe_load(f)

def main():
    params = load_params()

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    print("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    df = pd.read_excel(params['data']['raw_path'])

    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    print("ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    X = df.drop(params['features']['drop_columns'] + [params['features']['target']], axis=1)
    y = df[params['features']['target']]

    # ĞšĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…
    encoder = OneHotEncoder(drop='first', sparse_output=False)
    encoded_cols = encoder.fit_transform(X[params['features']['categorical_columns']])
    encoded_df = pd.DataFrame(
        encoded_cols,
        columns=encoder.get_feature_names_out(params['features']['categorical_columns'])
    )

    X_processed = pd.concat([X.drop(params['features']['categorical_columns'], axis=1), encoded_df], axis=1)

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y,
        test_size=params['preprocessing']['test_size'],
        random_state=params['preprocessing']['random_state']
    )

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
    os.makedirs('data/processed', exist_ok=True)
    joblib.dump({
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'feature_names': X_processed.columns.tolist()
    }, params['data']['processed_path'])

    os.makedirs('models', exist_ok=True)
    joblib.dump(encoder, 'models/encoder.joblib')
    joblib.dump(X_processed.columns.tolist(), 'models/feature_columns.joblib')

    print("âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹")

if __name__ == "__main__":
    main()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./src/register_model.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import mlflow
from mlflow.tracking import MlflowClient
import yaml
import json
import os
from datetime import datetime

def load_params():
    with open("params.yaml", "r") as f:
        return yaml.safe_load(f)

def main():
    params = load_params()

    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ° MLflow
    client = MlflowClient()

    try:
        # ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
        experiment = client.get_experiment_by_name("NPV_Prediction_DVC")
        if not experiment:
            print("âŒ Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹
            registry_info = {"error": "Experiment not found"}
            os.makedirs('registry', exist_ok=True)
            with open('registry/model_info.json', 'w') as f:
                json.dump(registry_info, f, indent=2)
            return

        runs = client.search_runs(experiment.experiment_id, order_by=["attributes.start_time DESC"])

        if runs:
            latest_run = runs[0]
            run_id = latest_run.info.run_id

            # Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            model_name = f"{params['model']['name']}_NPV"

            try:
                # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
                client.get_registered_model(model_name)
                print(f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_name} ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚")
                model_status = "existing"
            except:
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
                client.create_registered_model(model_name)
                print(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {model_name}")
                model_status = "new"

            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ĞµÑ€ÑĞ¸Ñ
            model_version = client.create_model_version(
                name=model_name,
                source=f"mlruns/{experiment.experiment_id}/{run_id}/artifacts/model",
                run_id=run_id
            )

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
            registry_info = {
                "model_name": model_name,
                "model_version": model_version.version,
                "run_id": run_id,
                "status": model_status,
                "timestamp": datetime.now().isoformat()  # Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ!
            }

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ registry ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
            os.makedirs('registry', exist_ok=True)

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ„Ğ°Ğ¹Ğ»
            with open('registry/model_info.json', 'w') as f:
                json.dump(registry_info, f, indent=2)

            print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_name} Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² MLflow Model Registry")
            print(f"ğŸ“ Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² registry/model_info.json")

        else:
            print("âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸")
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹
            os.makedirs('registry', exist_ok=True)
            with open('registry/model_info.json', 'w') as f:
                json.dump({"error": "No runs found"}, f, indent=2)

    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
        os.makedirs('registry', exist_ok=True)
        with open('registry/model_info.json', 'w') as f:
            json.dump({"error": str(e)}, f, indent=2)

if __name__ == "__main__":
    main()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./src/train.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import joblib
import json
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score
import yaml
import mlflow

def load_params():
    with open("params.yaml", "r") as f:
        return yaml.safe_load(f)

def main():
    params = load_params()

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    print("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    data = joblib.load(params['data']['processed_path'])
    X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']

    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° MLflow
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("NPV_Prediction_DVC")

    with mlflow.start_run():
        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        print("ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
        model = XGBRegressor(
            random_state=params['preprocessing']['random_state'],
            objective='reg:squarederror',
            **params['model']['hyperparameters']
        )

        model.fit(X_train, y_train)

        # ĞšÑ€Ğ¾ÑÑ-Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
        cv_scores = cross_val_score(
            model, X_train, y_train,
            cv=params['training']['cv_folds'],
            scoring=params['training']['scoring']
        )

        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        y_pred = model.predict(X_test)
        metrics = {
            'mae': mean_absolute_error(y_test, y_pred),
            'r2': r2_score(y_test, y_pred),
            'mape': mean_absolute_percentage_error(y_test, y_pred),
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² MLflow
        mlflow.log_params(params['model']['hyperparameters'])
        for metric, value in metrics.items():
            mlflow.log_metric(metric, value)

        mlflow.sklearn.log_model(model, "model")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
        joblib.dump(model, 'models/model.joblib')
        with open('models/metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ°. R2: {metrics['r2']:.4f}")

if __name__ == "__main__":
    main()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ï¿½ Ğ¤ĞĞ™Ğ›: ./streamlit_app.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import streamlit as st
import requests
import pandas as pd
import json
import os
from datetime import datetime

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
st.set_page_config(
    page_title="NPV Prediction App",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
st.title("ğŸ’° NPV Prediction App")
st.markdown("ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ NPV Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½Ñ‹")

# URL Ğ²Ğ°ÑˆĞµĞ³Ğ¾ API (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸)
API_URL = st.sidebar.text_input(
    "URL API",
    value=os.getenv('API_URL', 'http://localhost:8001'),  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ¾Ğ¼
    help="Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ URL Ğ²Ğ°ÑˆĞµĞ³Ğ¾ FastAPI ÑĞµÑ€Ğ²ĞµÑ€Ğ°"
)

# MLflow URL Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
MLFLOW_URL = st.sidebar.text_input(
    "MLflow URL",
    value=os.getenv('MLFLOW_URL', 'http://localhost:5000'),  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ¾Ğ¼
    help="Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ URL MLflow ÑĞµÑ€Ğ²ĞµÑ€Ğ°"
)

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ API
def check_api_health():
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°
def get_prediction(data):
    try:
        response = requests.post(
            f"{API_URL}/predict",
            json=data,
            timeout=10
        )
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"ĞÑˆĞ¸Ğ±ĞºĞ° API: {response.status_code} - {response.text}"}
    except Exception as e:
        return {"error": f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ: {str(e)}"}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
def get_model_info():
    try:
        response = requests.get(f"{API_URL}/model_info", timeout=5)
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except:
        return None

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸Ğ· DVC
def get_dvc_metrics():
    try:
        if os.path.exists('models/metrics.json'):
            with open('models/metrics.json', 'r') as f:
                return json.load(f)
        return None
    except:
        return None

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
def get_model_params():
    try:
        if os.path.exists('params.yaml'):
            import yaml
            with open('params.yaml', 'r') as f:
                return yaml.safe_load(f)
        return None
    except:
        return None

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ API
st.sidebar.header("ğŸ”— Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ")
if st.sidebar.button("ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ API"):
    if check_api_health():
        st.sidebar.success("âœ… API Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
    else:
        st.sidebar.error("âŒ API Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
st.header("ğŸ“ Ğ’Ğ²Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²")

col1, col2 = st.columns(2)

with col1:
    st.subheader("Ğ“ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹")
    heff = st.number_input("Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° (Heff)", min_value=0.0, value=10.0, step=0.1,
                          help="Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° Ğ¿Ğ»Ğ°ÑÑ‚Ğ°")
    perm = st.number_input("ĞŸÑ€Ğ¾Ğ½Ğ¸Ñ†Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ (Perm)", min_value=0.0, value=100.0, step=1.0,
                          help="ĞŸÑ€Ğ¾Ğ½Ğ¸Ñ†Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€Ğ¾Ğ´Ñ‹")
    sg = st.slider("Ğ“Ğ°Ğ·Ğ¾Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ (Sg)", min_value=0.0, max_value=1.0, value=0.8, step=0.01,
                  help="Ğ”Ğ¾Ğ»Ñ Ğ³Ğ°Ğ·Ğ° Ğ² Ğ¿Ğ»Ğ°ÑÑ‚Ğµ")
    c5 = st.number_input("Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ C5+", min_value=0.0, value=0.5, step=0.1,
                        help="Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ñ‚ÑĞ¶ĞµĞ»Ñ‹Ñ… ÑƒĞ³Ğ»ĞµĞ²Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²")

with col2:
    st.subheader("Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹")
    l_hor = st.number_input("Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° (L_hor)", min_value=0.0, value=500.0, step=10.0,
                           help="Ğ”Ğ»Ğ¸Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ°")
    gs = st.selectbox("Ğ¢Ğ¸Ğ¿ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ ÑÑ‚Ğ²Ğ¾Ğ»Ğ°", ["S-TYPE", "U-TYPE", "VGS", "GS", "NGS"],
                     help="ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½Ñ‹")
    temp = st.number_input("Ğ¢ĞµĞ¼Ğ¿ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ", min_value=0.0, value=20.0, step=0.1,
                          help="Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ° Ğ² Ğ¿Ğ»Ğ°ÑÑ‚Ğµ")
    grp = st.number_input("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¹ Ğ“Ğ ĞŸ", min_value=0, value=1, step=1,
                         help="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¹ Ğ³Ğ¸Ğ´Ñ€Ğ¾Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¿Ğ»Ğ°ÑÑ‚Ğ°")
    ngs = st.number_input("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ²Ğ¾Ğ»Ğ¾Ğ²", min_value=0, value=2, step=1,
                         help="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ĞµÑ‚Ğ²ĞµĞ¹ ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½Ñ‹")

# ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
if st.button("ğŸ¯ Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ NPV", type="primary"):
    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ API
    input_data = {
        "Heff": heff,
        "Perm": perm,
        "Sg": sg,
        "L_hor": l_hor,
        "GS": gs,
        "temp": temp,
        "C5": c5,
        "GRP": grp,
        "nGS": ngs
    }

    # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°
    with st.spinner("ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°..."):
        result = get_prediction(input_data)

    # ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    if "predicted_NPV" in result:
        # ĞšÑ€Ğ°ÑĞ¸Ğ²Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°
        st.success(f"## ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ NPV: **${result['predicted_NPV']:,.2f}**")

        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("NPV", f"${result['predicted_NPV']:,.2f}")
        with col2:
            # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            st.metric("Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ", "âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾")
        with col3:
            st.metric("Ğ’Ñ€ĞµĞ¼Ñ", datetime.now().strftime("%H:%M:%S"))

        # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
        with st.expander("ğŸ“Š Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°"):
            st.subheader("Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹")
            st.json(input_data)

            st.subheader("ĞÑ‚Ğ²ĞµÑ‚ API")
            st.json(result)

            # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ (Ğ² ÑĞµÑÑĞ¸Ğ¸)
            if 'prediction_history' not in st.session_state:
                st.session_state.prediction_history = []

            st.session_state.prediction_history.append({
                'timestamp': datetime.now().isoformat(),
                'input': input_data,
                'prediction': result['predicted_NPV']
            })

            st.subheader("Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹")
            if st.session_state.prediction_history:
                history_df = pd.DataFrame(st.session_state.prediction_history)
                st.dataframe(history_df)
    else:
        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {result.get('error', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°')}")

# Ğ‘Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ğ½ĞµĞ»ÑŒ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
st.sidebar.header("â„¹ï¸ Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ")

# Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· API
model_info = get_model_info()
if model_info and "error" not in model_info:
    st.sidebar.success("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
    st.sidebar.write(f"**Ğ¢Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:** {model_info.get('model_type', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾')}")
    st.sidebar.write(f"**ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:** {model_info.get('n_features', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾')}")

    if "features" in model_info:
        with st.sidebar.expander("Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²"):
            for feature in model_info["features"]:
                st.write(f"â€¢ {feature}")
else:
    st.sidebar.warning("âš ï¸ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")

# ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· DVC
dvc_metrics = get_dvc_metrics()
if dvc_metrics:
    with st.sidebar.expander("ğŸ“ˆ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (DVC)"):
        st.write(f"**RÂ²:** {dvc_metrics.get('r2', 'N/A'):.4f}")
        st.write(f"**MAE:** {dvc_metrics.get('mae', 'N/A'):.2f}")
        st.write(f"**MAPE:** {dvc_metrics.get('mape', 'N/A'):.2%}")
        st.write(f"**CV RÂ²:** {dvc_metrics.get('cv_mean', 'N/A'):.4f} Â± {dvc_metrics.get('cv_std', 'N/A'):.4f}")

# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
model_params = get_model_params()
if model_params:
    with st.sidebar.expander("âš™ï¸ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"):
        st.write(f"**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼:** {model_params.get('model', {}).get('name', 'N/A')}")
        params = model_params.get('model', {}).get('hyperparameters', {})
        for key, value in params.items():
            st.write(f"**{key}:** {value}")

# Ğ¡ÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
st.sidebar.header("ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³")
if st.sidebar.button("ğŸ“ˆ ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ MLflow"):
    st.markdown(f'<a href="{MLFLOW_URL}" target="_blank">ğŸ“ˆ ĞŸĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğº MLflow</a>', unsafe_allow_html=True)

if st.sidebar.button("ğŸ”„ ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½"):
    with st.sidebar:
        with st.spinner("Ğ—Ğ°Ğ¿ÑƒÑĞº DVC Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°..."):
            import subprocess
            try:
                result = subprocess.run(['dvc', 'repro'], capture_output=True, text=True)
                if result.returncode == 0:
                    st.success("âœ… ĞŸĞ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½")
                else:
                    st.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {result.stderr}")
            except Exception as e:
                st.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°: {e}")

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
st.sidebar.header("ğŸ¯ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")

example_data = [
    {
        "name": "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 1: Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½Ğ°",
        "data": {"Heff": 15.0, "Perm": 150.0, "Sg": 0.75, "L_hor": 600.0,
                "GS": "S-TYPE", "temp": 25.0, "C5": 0.6, "GRP": 2, "nGS": 3}
    },
    {
        "name": "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 2: Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ",
        "data": {"Heff": 25.0, "Perm": 300.0, "Sg": 0.85, "L_hor": 800.0,
                "GS": "U-TYPE", "temp": 30.0, "C5": 0.7, "GRP": 3, "nGS": 4}
    }
]

# Ğ”Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ session_state Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ¹
if 'load_example' not in st.session_state:
    st.session_state.load_example = None

for i, example in enumerate(example_data):
    if st.sidebar.button(example["name"]):
        st.session_state.load_example = example["data"]
        st.rerun()  # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ app Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ¹

# Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ
if st.session_state.load_example:
    heff = st.session_state.load_example["Heff"]
    perm = st.session_state.load_example["Perm"]
    sg = st.session_state.load_example["Sg"]
    l_hor = st.session_state.load_example["L_hor"]
    gs = st.session_state.load_example["GS"]
    temp = st.session_state.load_example["temp"]
    c5 = st.session_state.load_example["C5"]
    grp = st.session_state.load_example["GRP"]
    ngs = st.session_state.load_example["nGS"]
    st.session_state.load_example = None  # Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸

# Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ
with st.expander("ğŸ“– Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"):
    st.markdown("""
    ### ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

    1. **Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ FastAPI ÑĞµÑ€Ğ²ĞµÑ€**: `python app.py` (Ğ¿Ğ¾Ñ€Ñ‚ 8001)
    2. **Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ MLflow**: `mlflow server --backend-store-uri file:mlruns --host localhost --port 5000`
    3. **Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹** ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
    4. **ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ 'Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ NPV'** Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°

    ### ğŸ”§ Ğ”Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²

    - **DVC Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½**: `dvc repro` - Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    - **ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸**: `dvc metrics show` - Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
    - **Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹**: `dvc exp run` - Ğ·Ğ°Ğ¿ÑƒÑĞº ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²

    ### ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

    - **MLflow**: ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    - **DVC**: Ğ’ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
    - **FastAPI**: Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ API Ğ¿Ğ¾ `/docs`

    **ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ:** Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹!
    """)

# Ğ¤ÑƒÑ‚ĞµÑ€
st.markdown("---")
st.caption(f"""
NPV Prediction App â€¢ Powered by FastAPI + Streamlit + XGBoost + DVC + MLflow
â€¢ Ğ’ĞµÑ€ÑĞ¸Ñ: 2.0 â€¢ {datetime.now().year}
""")



docker-compose.yml
services:
  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ MLflow Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    command: mlflow server --backend-store-uri sqlite:////mlflow/mlflow.db --default-artifact-root /mlflow/artifacts --host 0.0.0.0 --port 5000
    volumes:
      - mlflow_data:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000

  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ FastAPI Ğ´Ğ»Ñ API Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ NPV
  api:
    build: .  # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ· Ğ¸Ğ· Dockerfile Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    ports:
      - "8001:8001"  # ĞŸĞ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº API (http://localhost:8001/docs Ğ´Ğ»Ñ Swagger)
    command: >  # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¾: Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ DVC Ğ¿ĞµÑ€ĞµĞ´ API
      bash -c "
        echo 'Ğ—Ğ°Ğ¿ÑƒÑĞº DVC Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°...' &&
        dvc repro &&  # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²ĞµÑÑŒ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ DVC
        echo 'DVC Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½. Ğ—Ğ°Ğ¿ÑƒÑĞº API...' &&
        uvicorn app:app --host 0.0.0.0 --port 8001  # Ğ—Ğ°Ğ¿ÑƒÑĞº FastAPI
      "
    depends_on:
      - mlflow  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ MLflow (Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ¿Ğ° register)
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000  # Ğ”Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² MLflow Ğ¸Ğ· API
    volumes:
      - .:/app  # ĞœĞ¾Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ (Ğ´Ğ»Ñ DVC Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
      - /app/__pycache__  # Ğ˜ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºÑÑˆ Python


  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Streamlit Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°
  streamlit:
    build: .  # Ğ¢Ğ¾Ñ‚ Ğ¶Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·
    ports:
      - "8501:8501"  # ĞŸĞ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº UI (http://localhost:8501)
    command: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0  # Ğ—Ğ°Ğ¿ÑƒÑĞº Streamlit
    depends_on:
      - api  # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ API
    environment:
      - API_URL=http://api:8001  # Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğº API Ğ¸Ğ· Streamlit
      - MLFLOW_TRACKING_URI=http://mlflow:5000  # Ğ”Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ MLflow
    volumes:
      - .:/app  # ĞœĞ¾Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚
      - /app/__pycache__

volumes:
  mlflow_data:  # Volume Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MLflow (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ñ‚ĞµÑ€ÑĞ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ)

app.py
from fastapi import FastAPI, HTTPException
import joblib
import pandas as pd
from pydantic import BaseModel, Field
import logging
import os

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="NPV Prediction API", version="1.0")

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ· DVC-Ğ²ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
try:
    model = joblib.load('models/model.joblib')
    encoder = joblib.load('models/encoder.joblib')
    feature_columns = joblib.load('models/feature_columns.joblib')
    logger.info("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")
except Exception as e:
    logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
    # ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° - ÑÑ‚Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
    model = None
    encoder = None
    feature_columns = None

# ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
class InputData(BaseModel):
    Heff: float = Field(..., ge=0, description="Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»Ñ‰Ğ¸Ğ½Ğ°")
    Perm: float = Field(..., ge=0, description="ĞŸÑ€Ğ¾Ğ½Ğ¸Ñ†Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ")
    Sg: float = Field(..., ge=0, le=1, description="Ğ“Ğ°Ğ·Ğ¾Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ")
    L_hor: float = Field(..., ge=0, description="Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ°")
    GS: str = Field(..., description="Ğ¢Ğ¸Ğ¿ ÑÑ‚Ğ²Ğ¾Ğ»Ğ°")
    temp: float = Field(..., ge=0, description="Ğ¢ĞµĞ¼Ğ¿ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ")
    C5: float = Field(..., ge=0, description="Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ C5")
    GRP: int = Field(..., ge=0, description="Ğ“Ğ ĞŸ")
    nGS: int = Field(..., ge=0, description="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ²Ğ¾Ğ»Ğ¾Ğ²")

@app.get("/")
async def root():
    return {"message": "NPV Prediction API", "status": "active", "model_loaded": model is not None}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/predict")
async def predict(data: InputData):
    if model is None:
        raise HTTPException(status_code=503, detail="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.")
    
    try:
        input_dict = data.dict()
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ DataFrame Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾
        numeric_data = {k: v for k, v in input_dict.items() if k != 'GS'}
        
        # ĞšĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ GS
        gs_encoded = encoder.transform([[input_dict['GS']]])
        gs_columns = encoder.get_feature_names_out(['GS'])
        gs_data = dict(zip(gs_columns, gs_encoded[0]))
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        all_data = {**numeric_data, **gs_data}
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ DataFrame Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼
        input_processed = pd.DataFrame([all_data])[feature_columns]
        
        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
        prediction = model.predict(input_processed)
        result = float(prediction[0])
        
        return {"predicted_NPV": round(result, 2), "status": "success"}
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ: {str(e)}")

@app.get("/model_info")
async def model_info():
    """Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    try:
        features = []
        if hasattr(model, 'feature_names_in_'):
            features = model.feature_names_in_.tolist()
        elif hasattr(model, 'get_booster'):
            features = model.get_booster().feature_names
        
        return {
            "model_type": type(model).__name__,
            "n_features": len(features),
            "features": features
        }
    except Exception as e:
        return {"error": str(e)}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)